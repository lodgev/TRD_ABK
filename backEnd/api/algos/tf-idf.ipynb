{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import log2\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "football_news_df = pd.read_csv('../data/news_test.csv', encoding='latin1')\n",
    "matches_df = pd.read_csv('../data/matches.csv', encoding='latin1')\n",
    "users_df = pd.read_csv('../data/users.csv')\n",
    "bets_df = pd.read_csv('../data/bets.csv')\n",
    "liked_clubs_df = pd.read_csv('../data/liked_clubs.csv')\n",
    "clubs_df = pd.read_csv('../data/clubs.csv')\n",
    "feedback_file_path = '../data/feedback.csv'\n",
    "\n",
    "try:\n",
    "    feedback_df = pd.read_csv(feedback_file_path)\n",
    "except FileNotFoundError:\n",
    "    feedback_df = pd.DataFrame(columns=['user_id', 'news_id', 'action', 'rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo: tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "def preprocess_data(news_df):\n",
    "    news_df = news_df.dropna(subset=['Content', 'Title'])\n",
    "    news_df['Content'] = news_df['Content'].fillna('')\n",
    "    return news_df\n",
    "\n",
    "# generate test data\n",
    "def generate_test_data(users, news_ids, num_relevant=5):\n",
    "    test_data = []\n",
    "    for user in users:\n",
    "        relevant_items = random.sample(news_ids, num_relevant)\n",
    "        test_data.append({\"user_id\": user, \"relevant_items\": relevant_items})\n",
    "    \n",
    "    return pd.DataFrame(test_data)\n",
    "\n",
    "# algo for recommendation\n",
    "def recommend_articles(user_id, football_news_df, bets_df, feedback_df, liked_clubs_df, clubs_df, top_n=20):\n",
    "\n",
    "    # User's preferred teams from bets\n",
    "    user_bets = bets_df[bets_df['user_id'] == user_id]\n",
    "    preferred_teams = user_bets['selected_team'].astype(str).unique() \n",
    "\n",
    "    # User's liked clubs\n",
    "    user_liked_clubs = liked_clubs_df[liked_clubs_df['user_id'] == user_id]['club_id'].tolist()\n",
    "\n",
    "    liked_clubs_info = clubs_df[clubs_df['id'].isin(user_liked_clubs)][['club', 'country']]\n",
    "    liked_club_names = liked_clubs_info['club'].astype(str).tolist()  \n",
    "    liked_club_countries = liked_clubs_info['country'].astype(str).tolist()  \n",
    "\n",
    "    preferred_articles = football_news_df[\n",
    "        football_news_df['Content'].str.contains('|'.join(preferred_teams), case=False, na=False)\n",
    "        | football_news_df['Content'].str.contains('|'.join(liked_club_names), case=False, na=False)\n",
    "        | football_news_df['Content'].str.contains('|'.join(liked_club_countries), case=False, na=False)\n",
    "    ]\n",
    "\n",
    "    # Exclude articles marked as \"not interested\"\n",
    "    not_interested = feedback_df[\n",
    "        (feedback_df['user_id'] == user_id) & (feedback_df['action'] == 'not_interested')\n",
    "    ]['news_id'].tolist()\n",
    "    filtered_articles = preferred_articles[~preferred_articles['News ID'].isin(not_interested)]\n",
    "\n",
    "    # TF-IDF Vectorization\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        stop_words='english', ngram_range=(1, 2), max_features=5000,\n",
    "        sublinear_tf=True, max_df=0.7, min_df=2\n",
    "    )\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(filtered_articles['Content'])\n",
    "\n",
    "    # Cosine Similarity\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Boost scores \n",
    "    user_feedback = feedback_df[feedback_df['user_id'] == user_id]\n",
    "    for _, feedback in user_feedback.iterrows():\n",
    "        if feedback['action'] == 'rated':\n",
    "            news_index = filtered_articles[filtered_articles['News ID'] == feedback['news_id']].index\n",
    "            if not news_index.empty:\n",
    "                news_idx = news_index[0]\n",
    "                if news_idx < similarity_matrix.shape[1]:\n",
    "                    similarity_matrix[:, news_idx] *= (1 + feedback['rating'] / 5.0)\n",
    "\n",
    "    # Rank and recommend\n",
    "    recommended_indices = similarity_matrix.sum(axis=1).argsort()[-top_n:][::-1]\n",
    "    recommendations = filtered_articles.iloc[recommended_indices]\n",
    "\n",
    "    return recommendations['News ID'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_news_df = preprocess_data(football_news_df)\n",
    "\n",
    "# Generate test data\n",
    "users = users_df['id'].tolist()\n",
    "news_ids = football_news_df['News ID'].tolist()\n",
    "test_data = generate_test_data(users, news_ids, num_relevant=5)\n",
    "\n",
    "# for evaluation system\n",
    "# Recommendation system wrapper\n",
    "class RecommendationSystem:\n",
    "    def recommend(self, user_id, k=10):\n",
    "        return recommend_articles(\n",
    "            user_id, football_news_df, bets_df, feedback_df, liked_clubs_df, clubs_df, top_n=k\n",
    "        )\n",
    "\n",
    "# Instantiate the system\n",
    "system = RecommendationSystem()\n",
    "\n",
    "# TF-IDF embeddings for diversity\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=5000)\n",
    "content_embeddings = tfidf_vectorizer.fit_transform(football_news_df['Content'])\n",
    "\n",
    "content_embedding_dict = {\n",
    "    row['News ID']: content_embeddings[i].toarray().flatten() for i, row in football_news_df.iterrows()\n",
    "}\n",
    "\n",
    "# Global popularity for novelty and serendipity\n",
    "global_popularity = {news_id: 1 / (i + 1) for i, news_id in enumerate(football_news_df['News ID'])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Evaluation of recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Evaluation functions ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Predictive quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended, relevant, k):\n",
    "    recommended_at_k = recommended[:k]\n",
    "    hits = len(set(recommended_at_k) & set(relevant))\n",
    "    return hits / k\n",
    "\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    recommended_at_k = recommended[:k]\n",
    "    hits = len(set(recommended_at_k) & set(relevant))\n",
    "    return hits / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "\n",
    "def f_score_at_k(precision, recall):\n",
    "    return (\n",
    "        2 * (precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ranking quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(recommended, relevant):\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def average_precision(recommended, relevant):\n",
    "    hits, precision_sum = 0, 0\n",
    "    for i, item in enumerate(recommended):\n",
    "        if item in relevant:\n",
    "            hits += 1\n",
    "            precision_sum += hits / (i + 1)\n",
    "    return precision_sum / len(relevant) if relevant else 0\n",
    "\n",
    "\n",
    "def ndcg(recommended, relevant, k):\n",
    "    recommended_at_k = recommended[:k]\n",
    "    dcg = sum(\n",
    "        [1 / log2(i + 2) if recommended_at_k[i] in relevant else 0 for i in range(k)]\n",
    "    )\n",
    "    idcg = sum([1 / log2(i + 2) for i in range(min(len(relevant), k))])\n",
    "    return dcg / idcg if idcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Behavioral metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity(recommended, content_embeddings):\n",
    "    embeddings = np.array([content_embeddings[news_id] for news_id in recommended if news_id in content_embeddings])\n",
    "    if len(embeddings) < 2: \n",
    "        return 0\n",
    "    \n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    return 1 - np.mean(similarities[np.triu_indices(len(similarities), k=1)])\n",
    "\n",
    "def novelty(recommended, global_popularity):\n",
    "    return np.mean([1 - global_popularity[item] for item in recommended])\n",
    "\n",
    "\n",
    "def serendipity(recommended, relevant, global_popularity):\n",
    "    unexpected_items = [item for item in recommended if item not in relevant]\n",
    "    return np.mean([1 - global_popularity[item] for item in unexpected_items])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## === Evaluation pipeline ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_system(system, test_data, content_embeddings, global_popularity, k=10):\n",
    "    precision_scores, recall_scores, f_scores = [], [], []\n",
    "    mrr_scores, map_scores, ndcg_scores = [], [], []\n",
    "    diversity_scores, novelty_scores, serendipity_scores = [], [], []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        relevant = set(row['relevant_items'])\n",
    "        recommended = system.recommend(user_id, k=k)\n",
    "\n",
    "        if not recommended:\n",
    "            continue\n",
    "\n",
    "        # Predictive metrics\n",
    "        precision = precision_at_k(recommended, relevant, k)\n",
    "        recall = recall_at_k(recommended, relevant, k)\n",
    "        f_score = f_score_at_k(precision, recall)\n",
    "\n",
    "        # Ranking metrics\n",
    "        mrr_score = mrr(recommended, relevant)\n",
    "        map_score = average_precision(recommended, relevant)\n",
    "        ndcg_score = ndcg(recommended, relevant, k)\n",
    "\n",
    "        # Behavioral metrics\n",
    "        diversity_score = diversity(recommended, content_embedding_dict)\n",
    "        novelty_score = novelty(recommended, global_popularity)\n",
    "        serendipity_score = serendipity(recommended, relevant, global_popularity)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f_scores.append(f_score)\n",
    "        mrr_scores.append(mrr_score)\n",
    "        map_scores.append(map_score)\n",
    "        ndcg_scores.append(ndcg_score)\n",
    "        diversity_scores.append(diversity_score)\n",
    "        novelty_scores.append(novelty_score)\n",
    "        serendipity_scores.append(serendipity_score)\n",
    "\n",
    "    return {\n",
    "        \"Precision@K\": np.mean(precision_scores) if precision_scores else 0,\n",
    "        \"Recall@K\": np.mean(recall_scores) if recall_scores else 0,\n",
    "        \"F-Score@K\": np.mean(f_scores) if f_scores else 0,\n",
    "        \"MRR\": np.mean(mrr_scores) if mrr_scores else 0,\n",
    "        \"MAP\": np.mean(map_scores) if map_scores else 0,\n",
    "        \"NDCG\": np.mean(ndcg_scores) if ndcg_scores else 0,\n",
    "        \"Diversity\": np.mean(diversity_scores) if diversity_scores else 0,\n",
    "        \"Novelty\": np.mean(novelty_scores) if novelty_scores else 0,\n",
    "        \"Serendipity\": np.mean(serendipity_scores) if serendipity_scores else 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual column names: ['user_id', 'relevant_items']\n",
      "Evaluation results:\n",
      "Precision@K: 0.0100\n",
      "Recall@K: 0.0200\n",
      "F-Score@K: 0.0133\n",
      "MRR: 0.0200\n",
      "MAP: 0.0040\n",
      "NDCG: 0.0131\n",
      "Diversity: 0.7778\n",
      "Novelty: 0.9874\n",
      "Serendipity: 0.9874\n"
     ]
    }
   ],
   "source": [
    "print(f\"Actual column names: {list(test_data.columns)}\")\n",
    "\n",
    "results = evaluate_system(system, test_data, content_embedding_dict, global_popularity, k=10)\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for metric, score in results.items():\n",
    "    print(f\"{metric}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
